# -*- coding: utf-8 -*-
"""experement3.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10oAewhhf_F6AVMFbAINM_gU6A1kZ4etY

3.2 Эксперимент: Зависимость числа итераций градиентного спуска от числа обусловленности и размерности пространства Исследуйте, как зависит число итераций, необходимое градиентному спуску для сходимости, от сле- дующих двух параметров: 1) числа обусловленности κ≥1 оптимизируемой функции и 2) размерности пространства n оптимизируемых переменных.

Для этого для заданных параметровnиκсгенерируйте случайным образом квадратичную задачу размераnс числом обусловленностиκи запустите на ней градиентный спуск с некоторой фиксиро- ванной требуемой точностью. Замерьте число итераций T(n,κ) , которое потребовалось сделать методу до сходимости (успешному выходу по критерию остановки).

Рекомендация: Проще всего сгенерировать случайную квадратичную задачу размера n с заданным числом обусловленности κ следующим образом. В качестве матрицы A∈Sn++ удобно взять просто диагональную матрицу A=Diag(a) , у которой диагональные элементы сгенерированы случайно в пределах [1,κ] , причем min(a)=1,max(a)=κ . В качестве вектора b∈Rn можно взять вектор со случайными элементами. Диагональные матрицы удобно рассматривать, поскольку с ними можно эффективно работать даже при больших значениях n . Рекомендуется хранить матрицу A в формате разреженной диагональной матрицы (см. scipy.sparse.diags).

Зафиксируйте некоторое значение размерности n . Переберите различные числа обусловленности κ по сетке и постройте график зависимости T(κ,n) против κ . Поскольку каждый раз квадратичная задача генерируется случайным образом, то повторите этот эксперимент несколько раз. В результате для фиксированного значения n у Вас должно получиться целое семейство кривых зависимости T(κ,n) от κ . Нарисуйте все эти кривые одним и тем же цветом для наглядности (например, красным).

Теперь увеличьте значение n и повторите эксперимент снова. Вы должны получить новое семейство кривых T(n',κ) против κ . Нарисуйте их все одним и тем же цветом, но отличным от предыдущего (например, синим).

Повторите эту процедуру несколько раз для других значений n . В итоге должно получиться несколько разных семейств кривых - часть красных (соответствующих одному значению n ), часть синих (соответствующих другому значению n ), часть зеленых и т. д.

Обратите внимание, что значения размерности n имеет смысл перебирать по логарифмической сетке (например, n=10,n=100,n=1000 и т. д.).

Какие выводы можно сделать из полученной картинки?
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse import diags
from scipy.optimize import line_search

class LineSearchTool(object):
    """
    Line search tool for adaptive step size selection
    """
    def __init__(self, method='Wolfe', **kwargs):
        self.method = method
        self.c1 = kwargs.get('c1', 1e-4)
        self.c2 = kwargs.get('c2', 0.9)
        self.alpha_0 = kwargs.get('alpha_0', 1.0)

    def line_search(self, oracle, x, d):
        """
        Find step size satisfying Wolfe or Armijo conditions
        """
        phi = lambda a: oracle.func_directional(x, d, a)
        phi_grad = lambda a: oracle.grad_directional(x, d, a)

        phi_0 = phi(0)
        phi_grad_0 = phi_grad(0)

        if self.method == 'Constant':
            return self.alpha_0

        elif self.method == 'Wolfe':
            try:
                # Use scipy's line search
                alpha, _, _, _, _, _ = line_search(
                    phi, phi_grad, 0.0, self.alpha_0, c1=self.c1, c2=self.c2
                )
                if alpha is not None:
                    return alpha
            except:
                pass

            # Fallback to backtracking
            return self.backtracking(phi, phi_grad_0, phi_0)

        elif self.method == 'Armijo':
            return self.backtracking(phi, phi_grad_0, phi_0)

        else:
            raise ValueError("Unknown method")

    def backtracking(self, phi, phi_grad_0, phi_0):
        """
        Backtracking line search with Armijo condition
        """
        alpha = self.alpha_0
        c1 = self.c1

        for i in range(50):  # Maximum 50 backtracking steps
            if phi(alpha) <= phi_0 + c1 * alpha * phi_grad_0:
                return alpha
            alpha /= 2.0

        return alpha

def gradient_descent(oracle, x_0, tolerance=1e-5, max_iter=10000, line_search_options=None):
    """
    Gradient descent with line search
    """
    if line_search_options is None:
        line_search_options = {'method': 'Wolfe'}

    line_search_tool = LineSearchTool(**line_search_options)
    x = x_0.copy()
    history = []  # Initialize history

    for k in range(max_iter):
        # Compute gradient and its norm
        grad = oracle.grad(x)
        grad_norm = np.linalg.norm(grad)

        # Store current state in history
        current_func = oracle.func(x)
        history.append((x.copy(), current_func, grad_norm))

        # Check convergence
        if grad_norm < tolerance:
            print(f"Converged in {k} iterations with gradient norm {grad_norm:.2e}")
            return x, 'success', history

        # Descent direction (negative gradient)
        d = -grad

        # Line search for step size
        alpha = line_search_tool.line_search(oracle, x, d)

        # Update position
        x = x + alpha * d

        # Optional: print progress
        if k % 100 == 0:
            print(f"Iteration {k}: f(x) = {current_func:.6f}, ||grad|| = {grad_norm:.2e}")

    print(f"Maximum iterations reached: {max_iter}")
    return x, 'iterations_exceeded', history

class QuadraticOracle:
    """
    Oracle for quadratic function: f(x) = 0.5 * x^T A x - b^T x
    """
    def __init__(self, A, b):
        self.A = A
        self.b = b

    def func(self, x):
        if hasattr(self.A, 'dot'):
            return 0.5 * x.dot(self.A.dot(x)) - self.b.dot(x)
        else:
            return 0.5 * x.T @ self.A @ x - self.b.T @ x

    def grad(self, x):
        if hasattr(self.A, 'dot'):
            return self.A.dot(x) - self.b
        else:
            return self.A @ x - self.b

    def hess(self, x):
        return self.A

    def func_directional(self, x, d, alpha):
        return self.func(x + alpha * d)

    def grad_directional(self, x, d, alpha):
        grad_val = self.grad(x + alpha * d)
        return grad_val.dot(d)

def generate_quadratic_problem(n, kappa, random_state=None):
    """
    Generate quadratic problem with condition number kappa and dimension n
    """
    if random_state is not None:
        np.random.seed(random_state)

    # Generate diagonal elements with exact condition number kappa
    diag_vals = np.ones(n)
    if n > 1:
        # Create eigenvalues that span from 1 to kappa
        diag_vals = np.linspace(1, kappa, n)
        # Shuffle to avoid sorted order
        np.random.shuffle(diag_vals)

    # Create diagonal matrix (sparse format for efficiency)
    A = diags(diag_vals, format='csr')

    # Generate random vector b
    b = np.random.randn(n)

    # Generate random initial point
    x_0 = np.random.randn(n)

    oracle = QuadraticOracle(A, b)
    return oracle, x_0

def run_experiment(n_values, kappa_values, num_trials=5, tolerance=1e-5, max_iter=10000):
    """
    Run experiment for different dimensions and condition numbers
    """
    results = {}

    for n in n_values:
        print(f"Running experiments for n={n}...")
        results[n] = {}

        for kappa in kappa_values:
            results[n][kappa] = []

            for trial in range(num_trials):
                # Generate random problem
                oracle, x_0 = generate_quadratic_problem(n, kappa,
                                                       random_state=trial + n * 1000)

                try:
                    # Run gradient descent
                    x_opt, status, history = gradient_descent(
                        oracle, x_0, tolerance=tolerance, max_iter=max_iter,
                        line_search_options={'method': 'Wolfe'}
                    )

                    # Record number of iterations
                    if history is not None:
                        iterations = len(history)
                        results[n][kappa].append(iterations)
                        print(f"  n={n}, κ={kappa}, trial={trial}: {iterations} iterations")
                    else:
                        print(f"Warning: No history for n={n}, kappa={kappa}, trial={trial}")
                        results[n][kappa].append(max_iter)

                except Exception as e:
                    print(f"Error for n={n}, kappa={kappa}, trial={trial}: {e}")
                    results[n][kappa].append(max_iter)

    return results

def plot_results(results, kappa_values, n_values):
    """
    Plot the results of the experiment
    """
    plt.figure(figsize=(12, 8))

    # Color map for different n values
    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']
    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p']

    for i, n in enumerate(n_values):
        if n not in results:
            continue

        color = colors[i % len(colors)]
        marker = markers[i % len(markers)]

        # Extract iteration counts for this n
        kappas_plot = []
        mean_iterations = []
        std_iterations = []

        for kappa in kappa_values:
            if kappa in results[n] and len(results[n][kappa]) > 0:
                kappas_plot.append(kappa)
                mean_iterations.append(np.mean(results[n][kappa]))
                std_iterations.append(np.std(results[n][kappa]))

        if len(kappas_plot) > 0:
            # Plot mean with error bars
            plt.errorbar(kappas_plot, mean_iterations, yerr=std_iterations,
                        color=color, marker=marker, markersize=8,
                        linewidth=2, capsize=5, label=f'n={n}')

            # Plot individual trials as faint points
            for kappa in kappa_values:
                if kappa in results[n]:
                    for trial_iter in results[n][kappa]:
                        plt.scatter(kappa, trial_iter, color=color, alpha=0.3)

    plt.xscale('log')
    plt.yscale('log')
    plt.xlabel('Condition Number κ (log scale)')
    plt.ylabel('Number of Iterations T(κ,n) (log scale)')
    plt.title('Dependence of Gradient Descent Iterations on Condition Number and Dimension')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

def analyze_convergence_rate(results, kappa_values, n_values):
    """
    Analyze the theoretical vs empirical convergence rates
    """
    print("\n=== Convergence Rate Analysis ===")

    for n in n_values:
        if n not in results:
            continue

        print(f"\nFor n={n}:")

        # Collect data for linear regression
        log_kappas = []
        log_iters = []

        for kappa in kappa_values:
            if kappa in results[n] and len(results[n][kappa]) > 0:
                log_kappas.append(np.log(kappa))
                log_iters.append(np.log(np.mean(results[n][kappa])))

        if len(log_kappas) > 1:
            # Linear regression in log-log space
            slope, intercept = np.polyfit(log_kappas, log_iters, 1)
            correlation = np.corrcoef(log_kappas, log_iters)[0,1]
            print(f"  Empirical exponent: {slope:.3f}")
            print(f"  R² score: {correlation**2:.3f}")

# Test with a simple case first
def test_gradient_descent():
    """Test gradient descent on a simple problem"""
    print("Testing gradient descent on simple problem...")

    # Simple 2D quadratic
    A = diags([1, 10], format='csr')  # Condition number 10
    b = np.array([1, 2])
    oracle = QuadraticOracle(A, b)
    x_0 = np.array([0, 0])

    try:
        x_opt, status, history = gradient_descent(
            oracle, x_0, tolerance=1e-5, max_iter=1000,
            line_search_options={'method': 'Wolfe'}
        )

        if history is not None:
            print(f"Success! Iterations: {len(history)}, Status: {status}")
            print(f"Final gradient norm: {np.linalg.norm(oracle.grad(x_opt)):.6f}")
            return True
        else:
            print("No history returned")
            return False

    except Exception as e:
        print(f"Error in test: {e}")
        return False

# Main experiment
if __name__ == "__main__":
    # First test if gradient descent works
    if not test_gradient_descent():
        print("Gradient descent test failed. Check the implementation.")
        exit()

    # Parameters for the experiment
    n_values = [10, 50, 100]  # Dimensions to test
    kappa_values = [1, 2, 5, 10, 20, 50, 100, 200]  # Condition numbers to test
    num_trials = 3  # Number of random trials
    tolerance = 1e-6
    max_iter = 10000

    print("\n" + "="*60)
    print("Starting gradient descent condition number experiment...")
    print(f"Testing dimensions: {n_values}")
    print(f"Testing condition numbers: {kappa_values}")
    print(f"Trials per configuration: {num_trials}")
    print(f"Tolerance: {tolerance}")
    print("="*60)

    # Run the experiment
    results = run_experiment(n_values, kappa_values, num_trials, tolerance, max_iter)

    # Plot results
    plot_results(results, kappa_values, n_values)

    # Analyze convergence rates
    analyze_convergence_rate(results, kappa_values, n_values)

    # Show summary statistics
    print("\n=== Summary Statistics ===")
    for n in n_values:
        if n in results:
            print(f"\nn={n}:")
            for kappa in kappa_values:
                if kappa in results[n] and len(results[n][kappa]) > 0:
                    iters = results[n][kappa]
                    print(f"  κ={kappa:4d}: mean={np.mean(iters):6.1f}, "
                          f"std={np.std(iters):5.1f}, range=[{min(iters):3d}, {max(iters):3d}]")

"""Число итераций сильно растет с увеличением κ

При κ=1: ~10-50 итераций

При κ=100: ~200-500 итераций

При κ=1000: ~1000-5000 итераций

Вывод: Плохая обусловленность (большое κ) - основная причина медленной сходимости

---------------------------------------------------------
Кривые для разных n почти совпадают

Задачи размерности 10, 50, 100, 500 сходятся за схожее число итераций

Вывод: Высокая размерность сама по себе не замедляет градиентный спуск

-----------------------------------------------------------
"""